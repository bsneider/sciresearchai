{
  "tasks": [
    {
      "id": "T-SEARCH-001",
      "title": "Implement Paper Search Subagent Core Infrastructure",
      "prd_traceability": {
        "feature_id": "F001",
        "prd_lines": [69, 76],
        "original_requirement": "Comprehensive literature discovery and retrieval across multiple databases"
      },
      "scope_boundaries": {
        "must_implement": [
          "Base SearchAgent class with coordination interfaces",
          "API integration framework for external databases",
          "Query formulation and optimization logic",
          "Relevance scoring and ranking algorithms",
          "Search result deduplication and filtering"
        ],
        "must_not_implement": [
          "Domain-specific search strategies",
          "Advanced citation network analysis",
          "User interface components"
        ],
        "out_of_scope_check": "BLOCK if implementing domain logic"
      },
      "documentation_context": {
        "primary_docs": [
          {
            "url": "https://www.semanticscholar.org/product/api",
            "version": "2025",
            "last_verified": "2025-11-09"
          }
        ],
        "version_locks": {
          "requests": "2.32.3",
          "pydantic": "2.9.2",
          "tenacity": "9.0.0"
        },
        "forbidden_patterns": ["Hardcoded API keys", "Synchronous blocking calls", "Unlimited retry loops"]
      },
      "hallucination_guards": {
        "verify_before_use": ["Semantic Scholar API endpoints", "arXiv API response formats", "Rate limiting patterns"],
        "forbidden_assumptions": ["No API key requirements assumed", "No unlimited rate limits assumed"]
      },
      "context_drift_prevention": {
        "task_boundaries": "This task ONLY implements core search infrastructure and API integration patterns",
        "refer_to_other_tasks": {
          "Quality filtering": "T-SEARCH-002",
          "Database integration": "T-SEARCH-003"
        },
        "max_file_changes": 6,
        "if_exceeds": "STOP and verify scope"
      },
      "milestone_metadata": {
        "milestone_id": "M1",
        "milestone_name": "Core Search Infrastructure",
        "is_milestone_critical": true,
        "can_defer": false,
        "milestone_position": 1
      }
    },
    {
      "id": "T-SEARCH-002",
      "title": "Build Scientific Database API Integrations",
      "prd_traceability": {
        "feature_id": "F001",
        "prd_lines": [135, 140],
        "original_requirement": "Query formulation across multiple databases (Semantic Scholar, arXiv, PubMed)"
      },
      "scope_boundaries": {
        "must_implement": [
          "Semantic Scholar API integration with rate limiting",
          "arXiv API connector with proper attribution",
          "PubMed/NCBI E-utilities integration via BioPython",
          "Unified query formatting for different APIs",
          "Error handling and retry logic with exponential backoff"
        ],
        "must_not_implement": [
          "Custom web scraping implementations",
          "付费数据库集成",
          "Real-time data synchronization"
        ],
        "out_of_scope_check": "BLOCK if implementing non-API data sources"
      },
      "documentation_context": {
        "primary_docs": [
          {
            "url": "https://biopython.org/docs/1.75/api/Bio.Entrez.html",
            "version": "1.75",
            "last_verified": "2025-11-09"
          }
        ],
        "version_locks": {
          "biopython": "1.84",
          "arxiv": "2.1.0",
          "ratelimit": "2.2.1"
        },
        "forbidden_patterns": ["Missing rate limiting", "Hardcoded retry counts", "No API key handling"]
      },
      "hallucination_guards": {
        "verify_before_use": ["BioPython Entrez usage patterns", "Semantic Scholar rate limits", "arXiv API terms"],
        "forbidden_assumptions": ["No API authentication required", "Unlimited request quotas"]
      },
      "context_drift_prevention": {
        "task_boundaries": "This task ONLY handles official API integrations with proper rate limiting",
        "refer_to_other_tasks": {
          "Core search logic": "T-SEARCH-001",
          "Quality assessment": "T-ANALYSIS-001"
        },
        "max_file_changes": 5,
        "if_exceeds": "STOP and verify scope"
      },
      "milestone_metadata": {
        "milestone_id": "M1",
        "milestone_name": "Core Search Infrastructure",
        "is_milestone_critical": true,
        "can_defer": false,
        "milestone_position": 1
      }
    },
    {
      "id": "T-SEARCH-003",
      "title": "Create Vector Search and Similarity System",
      "prd_traceability": {
        "feature_id": "F001",
        "prd_lines": [137, 138],
        "original_requirement": "Semantic and keyword-based search execution with relevance scoring"
      },
      "scope_boundaries": {
        "must_implement": [
          "Sentence Transformer integration with all-mpnet-base-v2",
          "ChromaDB vector storage and retrieval system",
          "Semantic similarity search algorithms",
          "Hybrid keyword + semantic search ranking",
          "Embedding generation and caching system"
        ],
        "must_not_implement": [
          "Custom embedding model training",
          "Advanced vector quantization",
          "Real-time embedding updates"
        ],
        "out_of_scope_check": "BLOCK if implementing custom ML models"
      },
      "documentation_context": {
        "primary_docs": [
          {
            "url": "https://docs.trychroma.com/",
            "version": "0.5.23",
            "last_verified": "2025-11-09"
          }
        ],
        "version_locks": {
          "chromadb": "0.5.23",
          "sentence-transformers": "3.3.1",
          "numpy": "2.1.3"
        },
        "forbidden_patterns": ["Manual embedding calculations", "Inefficient vector operations", "Missing dimension checks"]
      },
      "hallucination_guards": {
        "verify_before_use": ["ChromaDB collection operations", "Sentence transformer inference patterns", "Vector similarity metrics"],
        "forbidden_assumptions": ["No GPU acceleration required", "No embedding dimension mismatches"]
      },
      "context_drift_prevention": {
        "task_boundaries": "This task ONLY implements vector search using existing pre-trained models",
        "refer_to_other_tasks": {
          "API integration": "T-SEARCH-002",
          "Paper analysis": "T-ANALYSIS-002"
        },
        "max_file_changes": 5,
        "if_exceeds": "STOP and verify scope"
      },
      "milestone_metadata": {
        "milestone_id": "M1",
        "milestone_name": "Core Search Infrastructure",
        "is_milestone_critical": true,
        "can_defer": false,
        "milestone_position": 1
      }
    },
    {
      "id": "T-SEARCH-004",
      "title": "Implement Search Workflow Coordination",
      "prd_traceability": {
        "feature_id": "F005",
        "prd_lines": [126, 145],
        "original_requirement": "End-to-end search process with ranking, filtering, and coverage analysis"
      },
      "scope_boundaries": {
        "must_implement": [
          "Search workflow orchestration logic",
          "Multi-database query coordination",
          "Result aggregation and ranking system",
          "Coverage analysis and gap identification",
          "Search strategy optimization feedback loop"
        ],
        "must_not_implement": [
          "Advanced citation network traversal",
          "User preference learning",
          "Advanced personalization algorithms"
        ],
        "out_of_scope_check": "BLOCK if implementing learning systems"
      },
      "documentation_context": {
        "primary_docs": [
          {
            "url": "https://docs.crewai.com/",
            "version": "2025",
            "last_verified": "2025-11-09"
          }
        ],
        "version_locks": {
          "crewai": "0.75.0",
          "asyncio": "built-in",
          "logging": "built-in"
        },
        "forbidden_patterns": ["Blocking workflow operations", "Missing error handling", "No progress tracking"]
      },
      "hallucination_guards": {
        "verify_before_use": ["CrewAI task delegation patterns", "Async workflow coordination", "Result aggregation strategies"],
        "forbidden_assumptions": ["No database coordination complexity", "Instant search results"]
      },
      "context_drift_prevention": {
        "task_boundaries": "This task ONLY handles search workflow coordination using existing components",
        "refer_to_other_tasks": {
          "Core search": "T-SEARCH-001",
          "API integration": "T-SEARCH-002",
          "Vector search": "T-SEARCH-003"
        },
        "max_file_changes": 4,
        "if_exceeds": "STOP and verify scope"
      },
      "milestone_metadata": {
        "milestone_id": "M1",
        "milestone_name": "Core Search Infrastructure",
        "is_milestone_critical": true,
        "can_defer": false,
        "milestone_position": 1
      }
    },
    {
      "id": "T-VAL-M1",
      "title": "Validate Milestone 1: Core Search Infrastructure",
      "type": "validation",
      "milestone_id": "M1",
      "validation_steps": [
        "Test all three database API integrations with rate limiting",
        "Verify vector search accuracy and performance",
        "Execute end-to-end search workflow with sample queries",
        "Validate result ranking and filtering quality",
        "Generate search performance and coverage report",
        "PAUSE for human review"
      ],
      "success_criteria": {
        "api_integrations_work": true,
        "vector_search_accurate": true,
        "end_to_end_search_succeeds": true,
        "rate_limiting_respected": true,
        "search_quality_acceptable": true
      },
      "rollback_point": true
    },
    {
      "id": "T-ANALYSIS-001",
      "title": "Implement Paper Reader Subagent Infrastructure",
      "prd_traceability": {
        "feature_id": "F002",
        "prd_lines": [77, 84],
        "original_requirement": "Critical analysis and assessment of scientific papers"
      },
      "scope_boundaries": {
        "must_implement": [
          "Base PaperReaderAgent class with analysis interfaces",
          "PDF text extraction and parsing system",
          "Research paper structure recognition (abstract, methods, results)",
          "Quality assessment framework and criteria",
          "Key finding extraction algorithms"
        ],
        "must_not_implement": [
          "Domain-specific quality criteria",
          "Advanced statistical analysis of papers",
          "Citation impact calculations"
        ],
        "out_of_scope_check": "BLOCK if implementing domain-specific analysis"
      },
      "documentation_context": {
        "primary_docs": [
          {
            "url": "https://pypdf2.readthedocs.io/",
            "version": "3.0.1",
            "last_verified": "2025-11-09"
          }
        ],
        "version_locks": {
          "pypdf2": "3.0.1",
          "nltk": "3.9.1",
          "textstat": "0.7.3"
        },
        "forbidden_patterns": ["Unstructured text processing", "Missing PDF error handling", "Assumed paper formats"]
      },
      "hallucination_guards": {
        "verify_before_use": ["PDF parsing error handling", "Research paper section identification", "Quality metric calculation methods"],
        "forbidden_assumptions": ["All PDFs are readable", "Consistent paper structure", "No corrupted downloads"]
      },
      "context_drift_prevention": {
        "task_boundaries": "This task ONLY implements generic paper analysis infrastructure",
        "refer_to_other_tasks": {
          "Quality assessment": "T-ANALYSIS-002",
          "Insight extraction": "T-ANALYSIS-003"
        },
        "max_file_changes": 6,
        "if_exceeds": "STOP and verify scope"
      },
      "milestone_metadata": {
        "milestone_id": "M2",
        "milestone_name": "Analysis & Reading System",
        "is_milestone_critical": true,
        "can_defer": false,
        "milestone_position": 2
      }
    },
    {
      "id": "T-ANALYSIS-002",
      "title": "Build Quality Assessment and Methodology Evaluation",
      "prd_traceability": {
        "feature_id": "F002",
        "prd_lines": [80, 83],
        "original_requirement": "Quality assessment and methodology evaluation"
      },
      "scope_boundaries": {
        "must_implement": [
          "Research methodology evaluation framework",
          "Statistical validity assessment tools",
          "Bias detection algorithms for research papers",
          "Reproducibility evaluation criteria",
          "Quality scoring system with configurable thresholds"
        ],
        "must_not_implement": [
          "Domain-specific statistical tests",
          "Advanced bias detection models",
          "Peer review simulation"
        ],
        "out_of_scope_check": "BLOCK if implementing advanced statistical analysis"
      },
      "documentation_context": {
        "primary_docs": [
          {
            "url": "https://www.nature.com/articles/s41586-023-06667-6",
            "version": "2023",
            "last_verified": "2025-11-09"
          }
        ],
        "version_locks": {
          "scipy": "1.14.1",
          "statsmodels": "0.14.4",
          "sklearn": "1.5.2"
        },
        "forbidden_patterns": ["Hardcoded quality thresholds", "Missing statistical validation", "Assumed methodology compliance"]
      },
      "hallucination_guards": {
        "verify_before_use": ["Statistical test implementations", "Bias detection algorithms", "Quality scoring methodologies"],
        "forbidden_assumptions": ["Complete methodology information available", "No statistical calculation errors"]
      },
      "context_drift_prevention": {
        "task_boundaries": "This task ONLY implements generic quality assessment tools",
        "refer_to_other_tasks": {
          "Core analysis": "T-ANALYSIS-001",
          "Insight synthesis": "T-ANALYSIS-003"
        },
        "max_file_changes": 5,
        "if_exceeds": "STOP and verify scope"
      },
      "milestone_metadata": {
        "milestone_id": "M2",
        "milestone_name": "Analysis & Reading System",
        "is_milestone_critical": true,
        "can_defer": false,
        "milestone_position": 2
      }
    },
    {
      "id": "T-ANALYSIS-003",
      "title": "Create Key Finding Extraction and Summarization",
      "prd_traceability": {
        "feature_id": "F002",
        "prd_lines": [82, 83],
        "original_requirement": "Key finding extraction and summarization"
      },
      "scope_boundaries": {
        "must_implement": [
          "Key finding extraction algorithms",
          "Research paper summarization system",
          "Citation network analysis integration",
          "Cross-paper insight identification",
          "Structured data extraction from unstructured text"
        ],
        "must_not_implement": [
          "Advanced NLP model training",
          "Domain-specific entity recognition",
          "Real-time insight updating"
        ],
        "out_of_scope_check": "BLOCK if implementing custom ML models"
      },
      "documentation_context": {
        "primary_docs": [
          {
            "url": "https://huggingface.co/sentence-transformers",
            "version": "2025",
            "last_verified": "2025-11-09"
          }
        ],
        "version_locks": {
          "transformers": "4.46.2",
          "spacy": "3.8.2",
          "networkx": "3.4.2"
        },
        "forbidden_patterns": ["Manual text extraction rules", "Missing context preservation", "Loss of important findings"]
      },
      "hallucination_guards": {
        "verify_before_use": ["Text summarization methodologies", "Citation parsing algorithms", "Finding extraction validation"],
        "forbidden_assumptions": ["Complete paper text available", "No ambiguous findings", "Accurate citation extraction"]
      },
      "context_drift_prevention": {
        "task_boundaries": "This task ONLY implements generic extraction using existing NLP models",
        "refer_to_other_tasks": {
          "Quality assessment": "T-ANALYSIS-002",
          "Hypothesis generation": "T-HYPOTH-001"
        },
        "max_file_changes": 5,
        "if_exceeds": "STOP and verify scope"
      },
      "milestone_metadata": {
        "milestone_id": "M2",
        "milestone_name": "Analysis & Reading System",
        "is_milestone_critical": true,
        "can_defer": false,
        "milestone_position": 2
      }
    },
    {
      "id": "T-ANALYSIS-004",
      "title": "Implement Critical Assessment Workflow",
      "prd_traceability": {
        "feature_id": "F006",
        "prd_lines": [146, 165],
        "original_requirement": "Paper analysis and quality evaluation with insight extraction"
      },
      "scope_boundaries": {
        "must_implement": [
          "End-to-end paper analysis workflow orchestration",
          "Multi-paper batch processing capabilities",
          "Analysis result aggregation and reporting",
          "Quality comparison across papers",
          "Analysis pipeline error handling and recovery"
        ],
        "must_not_implement": [
          "Advanced peer review simulation",
          "Collaborative analysis workflows",
          "Real-time analysis updates"
        ],
        "out_of_scope_check": "BLOCK if implementing collaborative features"
      },
      "documentation_context": {
        "primary_docs": [
          {
            "url": "https://docs.celeryproject.org/",
            "version": "5.4.0",
            "last_verified": "2025-11-09"
          }
        ],
        "version_locks": {
          "celery": "5.4.0",
          "redis": "5.2.1",
          "asyncio": "built-in"
        },
        "forbidden_patterns": ["Blocking batch processing", "Missing progress tracking", "No error recovery"]
      },
      "hallucination_guards": {
        "verify_before_use": ["Celery task queue patterns", "Batch processing coordination", "Analysis result aggregation"],
        "forbidden_assumptions": ["All papers process successfully", "No batch processing bottlenecks"]
      },
      "context_drift_prevention": {
        "task_boundaries": "This task ONLY handles workflow orchestration using existing analysis components",
        "refer_to_other_tasks": {
          "Core analysis": "T-ANALYSIS-001",
          "Quality assessment": "T-ANALYSIS-002",
          "Finding extraction": "T-ANALYSIS-003"
        },
        "max_file_changes": 4,
        "if_exceeds": "STOP and verify scope"
      },
      "milestone_metadata": {
        "milestone_id": "M2",
        "milestone_name": "Analysis & Reading System",
        "is_milestone_critical": true,
        "can_defer": false,
        "milestone_position": 2
      }
    },
    {
      "id": "T-ANALYSIS-005",
      "title": "Build Cross-Reference and Citation Analysis",
      "prd_traceability": {
        "feature_id": "F002",
        "prd_lines": [83, 84],
        "original_requirement": "Citation network analysis"
      },
      "scope_boundaries": {
        "must_implement": [
          "Citation extraction from reference sections",
          "Citation network graph construction",
          "Cross-paper relationship identification,
          "Impact factor and citation metrics calculation,
          "Neo4j integration for citation storage"
        ],
        "must_not_implement": [
          "Advanced bibliometric analysis",
          "Historical citation trend analysis",
          "International collaboration mapping"
        ],
        "out_of_scope_check": "BLOCK if implementing advanced bibliometrics"
      },
      "documentation_context": {
        "primary_docs": [
          {
            "url": "https://neo4j.com/docs/",
            "version": "5.24.2",
            "last_verified": "2025-11-09"
          }
        ],
        "version_locks": {
          "neo4j": "5.24.2",
          "pyvis": "0.3.2",
          "citation_parser": "0.0.1"
        },
        "forbidden_patterns": ["Manual citation parsing", "Missing reference format handling", "Inconsistent citation data"]
      },
      "hallucination_guards": {
        "verify_before_use": ["Neo4j graph database operations", "Citation parsing algorithms", "Network analysis methods"],
        "forbidden_assumptions": ["Consistent citation formats", "Complete reference information", "No parsing errors"]
      },
      "context_drift_prevention": {
        "task_boundaries": "This task ONLY implements basic citation network analysis",
        "refer_to_other_tasks": {
          "Finding extraction": "T-ANALYSIS-003",
          "Hypothesis generation": "T-HYPOTH-001"
        },
        "max_file_changes": 5,
        "if_exceeds": "STOP and verify scope"
      },
      "milestone_metadata": {
        "milestone_id": "M2",
        "milestone_name": "Analysis & Reading System",
        "is_milestone_critical": true,
        "can_defer": false,
        "milestone_position": 2
      }
    },
    {
      "id": "T-VAL-M2",
      "title": "Validate Milestone 2: Analysis & Reading System",
      "type": "validation",
      "milestone_id": "M2",
      "validation_steps": [
        "Test paper parsing with diverse PDF formats",
        "Validate quality assessment accuracy on sample papers",
        "Execute batch processing workflows with multiple papers",
        "Verify citation network construction and analysis",
        "Generate analysis quality and performance report",
        "PAUSE for human review"
      ],
      "success_criteria": {
        "pdf_parsing_accurate": true,
        "quality_assessment_reliable": true,
        "batch_processing_efficient": true,
        "citation_analysis_correct": true,
        "analysis_workflow_complete": true
      },
      "rollback_point": true
    },
    {
      "id": "T-HYPOTH-001",
      "title": "Implement Hypothesis Subagent Core Infrastructure",
      "prd_traceability": {
        "feature_id": "F003",
        "prd_lines": [85, 91],
        "original_requirement": "Novel hypothesis generation from synthesized knowledge"
      },
      "scope_boundaries": {
        "must_implement": [
          "Base HypothesisGeneratorAgent class,
          "Cross-paper knowledge synthesis algorithms",
          "Research gap identification framework",
          "Testable hypothesis formulation logic,
          "Evidence-based hypothesis validation system"
        ],
        "must_not_implement": [
          "Domain-specific hypothesis templates",
          "Advanced experimental design generation",
          "Creative insight generation beyond synthesis"
        ],
        "out_of_scope_check": "BLOCK if implementing domain-specific hypotheses"
      },
      "documentation_context": {
        "primary_docs": [
          {
            "url": "https://huggingface.co/docs/transformers/",
            "version": "4.46.2",
            "last_verified": "2025-11-09"
          }
        ],
        "version_locks": {
          "transformers": "4.46.2",
          "networkx": "3.4.2",
          "itertools": "built-in"
        },
        "forbidden_patterns": ["Random hypothesis generation", "Missing evidence validation", "Unsynthesized claims"]
      },
      "hallucination_guards": {
        "verify_before_use": ["Knowledge synthesis methodologies", "Gap detection algorithms", "Hypothesis validation frameworks"],
        "forbidden_assumptions": ["Complete insight coverage", "No contradictory evidence", "Valid gap identification"]
      },
      "context_drift_prevention": {
        "task_boundaries": "This task ONLY implements hypothesis generation from analyzed papers",
        "refer_to_other_tasks": {
          "Paper analysis": "T-ANALYSIS-003",
          "Gap analysis": "T-HYPOTH-002"
        },
        "max_file_changes": 5,
        "if_exceeds": "STOP and verify scope"
      },
      "milestone_metadata": {
        "milestone_id": "M3",
        "milestone_name": "Hypothesis Generation",
        "is_milestone_critical": true,
        "can_defer": false,
        "milestone_position": 3
      }
    },
    {
      "id": "T-HYPOTH-002",
      "title": "Build Research Gap Analysis System",
      "prd_traceability": {
        "feature_id": "F003",
        "prd_lines": [89, 90],
        "original_requirement": "Gap identification in current research"
      },
      "scope_boundaries": {
        "must_implement": [
          "Research landscape analysis algorithms",
          "Unexplored area identification methods",
          "Trend analysis across paper collections,
          "Methodology gap detection framework,
          "Opportunity mapping for new research"
        ],
        "must_not_implement": [
          "Advanced trend prediction models",
          "Market analysis for research opportunities",
          "Funding opportunity identification"
        ],
        "out_of_scope_check": "BLOCK if implementing market analysis"
      },
      "documentation_context": {
        "primary_docs": [
          {
            "url": "https://scikit-learn.org/",
            "version": "1.5.2",
            "last_verified": "2025-11-09"
          }
        ],
        "version_locks": {
          "scikit-learn": "1.5.2",
          "pandas": "2.2.3",
          "matplotlib": "3.9.3"
        },
        "forbidden_patterns": ["Manual gap identification", "Missing trend analysis", "Assumed complete coverage"]
      },
      "hallucination_guards": {
        "verify_before_use": ["Trend analysis algorithms", "Gap detection methodologies", "Research landscape mapping"],
        "forbidden_assumptions": ["Complete literature coverage", "No hidden research areas", "Accurate trend identification"]
      },
      "context_drift_prevention": {
        "task_boundaries": "This task ONLY implements gap analysis using existing analytical tools",
        "refer_to_other_tasks": {
          "Hypothesis generation": "T-HYPOTH-001",
          "Knowledge synthesis": "T-HYPOTH-003"
        },
        "max_file_changes": 4,
        "if_exceeds": "STOP and verify scope"
      },
      "milestone_metadata": {
        "milestone_id": "M3",
        "milestone_name": "Hypothesis Generation",
        "is_milestone_critical": true,
        "can_defer": false,
        "milestone_position": 3
      }
    },
    {
      "id": "T-HYPOTH-003",
      "title": "Create Knowledge Synthesis Engine",
      "prd_traceability": {
        "feature_id": "F003",
        "prd_lines": [88, 89],
        "original_requirement": "Cross-paper insight synthesis"
      },
      "scope_boundaries": {
        "must_implement": [
          "Multi-paper insight combination algorithms",
          "Pattern recognition across research findings,
          "Methodology synthesis framework,
          "Concept integration from multiple sources,
          "Synthesis quality assessment system"
        ],
        "must_not_implement": [
          "Creative idea generation",
          "Novel concept creation beyond synthesis",
          "Cross-domain analogy generation"
        ],
        "out_of_scope_check": "BLOCK if implementing creative generation"
      },
      "documentation_context": {
        "primary_docs": [
          {
            "url": "https://networkx.org/",
            "version": "3.4.2",
            "last_verified": "2025-11-09"
          }
        ],
        "version_locks": {
          "networkx": "3.4.2",
          "numpy": "2.1.3",
          "scipy": "1.14.1"
        },
        "forbidden_patterns": ["Random insight combination", "Missing quality validation", "Unvalidated synthesis"]
      },
      "hallucination_guards": {
        "verify_before_use": ["Graph theory applications", "Insight combination methodologies", "Quality assessment algorithms"],
        "forbidden_assumptions": ["Valid insight connections", "No contradictory synthesis", "Complete integration possible"]
      },
      "context_drift_prevention": {
        "task_boundaries": "This task ONLY implements synthesis of existing research findings",
        "refer_to_other_tasks": {
          "Gap analysis": "T-HYPOTH-002",
          "Hypothesis formulation": "T-HYPOTH-001"
        },
        "max_file_changes": 4,
        "if_exceeds": "STOP and verify scope"
      },
      "milestone_metadata": {
        "milestone_id": "M3",
        "milestone_name": "Hypothesis Generation",
        "is_milestone_critical": true,
        "can_defer": false,
        "milestone_position": 3
      }
    },
    {
      "id": "T-HYPOTH-004",
      "title": "Implement Knowledge Synthesis & Hypothesis Workflow",
      "prd_traceability": {
        "feature_id": "F007",
        "prd_lines": [166, 185],
        "original_requirement": "Cross-paper insight synthesis and novel hypothesis creation"
      },
      "scope_boundaries": {
        "must_implement": [
          "End-to-end hypothesis generation workflow,
          "Multi-step synthesis and validation pipeline,
          "Hypothesis ranking and prioritization system,
          "Evidence gathering and validation framework,
          "Research roadmap generation from hypotheses"
        ],
        "must_not_implement": [
          "Experimental protocol design",
          "Funding proposal generation",
          "Collaborative hypothesis development"
        ],
        "out_of_scope_check": "BLOCK if implementing experimental design"
      },
      "documentation_context": {
        "primary_docs": [
          {
            "url": "https://docs.crewai.com/",
            "version": "2025",
            "last_verified": "2025-11-09"
          }
        ],
        "version_locks": {
          "crewai": "0.75.0",
          "asyncio": "built-in",
          "logging": "built-in"
        },
        "forbidden_patterns": ["Single-step hypothesis generation", "Missing validation steps", "No evidence tracking"]
      },
      "hallucination_guards": {
        "verify_before_use": ["Multi-step workflow orchestration", "Hypothesis validation methodologies", "Evidence tracking systems"],
        "forbidden_assumptions": ["Valid hypothesis generation", "Complete evidence coverage", "No logical gaps"]
      },
      "context_drift_prevention": {
        "task_boundaries": "This task ONLY orchestrates the complete hypothesis generation pipeline",
        "refer_to_other_tasks": {
          "Core generation": "T-HYPOTH-001",
          "Gap analysis": "T-HYPOTH-002",
          "Knowledge synthesis": "T-HYPOTH-003"
        },
        "max_file_changes": 4,
        "if_exceeds": "STOP and verify scope"
      },
      "milestone_metadata": {
        "milestone_id": "M3",
        "milestone_name": "Hypothesis Generation",
        "is_milestone_critical": true,
        "can_defer": false,
        "milestone_position": 3
      }
    },
    {
      "id": "T-VAL-M3",
      "title": "Validate Milestone 3: Hypothesis Generation",
      "type": "validation",
      "milestone_id": "M3",
      "validation_steps": [
        "Test hypothesis generation with analyzed paper sets",
        "Validate gap identification accuracy on known research areas",
        "Execute knowledge synthesis with sample research topics",
        "Verify hypothesis quality and testability assessment",
        "Generate hypothesis generation quality report",
        "PAUSE for human review"
      ],
      "success_criteria": {
        "hypotheses_generated": true,
        "gaps_identified_accurately": true,
        "knowledge_synthesis_effective": true,
        "hypotheses_testable": true,
        "workflow_complete": true
      },
      "rollback_point": true
    },
    {
      "id": "T-CMD-001",
      "title": "Build Claude Code Command System Integration",
      "prd_traceability": {
        "feature_id": "F004",
        "prd_lines": [107, 120],
        "original_requirement": "8 core research slash commands for Claude Code"
      },
      "scope_boundaries": {
        "must_implement": [
          "Slash command parser and router system",
          "/research:search command implementation",
          "/research:analyze command implementation",
          "/research:synthesize command implementation",
          "/research:hypothesize command implementation"
        ],
        "must_not_implement": [
          "Additional research commands beyond core 8",
          "Advanced command customization features",
          "User preference learning"
        ],
        "out_of_scope_check": "BLOCK if implementing non-core commands"
      },
      "documentation_context": {
        "primary_docs": [
          {
            "url": "https://docs.anthropic.com/claude/docs/claude-code",
            "version": "2025",
            "last_verified": "2025-11-09"
          }
        ],
        "version_locks": {
          "anthropic": "0.40.0",
          "asyncio": "built-in",
          "json": "built-in"
        },
        "forbidden_patterns": ["Blocking command execution", "Missing error handling", "No progress feedback"]
      },
      "hallucination_guards": {
        "verify_before_use": ["Claude Code command integration patterns", "Async command execution", "Progress reporting mechanisms"],
        "forbidden_assumptions": ["Claude Code availability", "Unlimited command execution time", "No command conflicts"]
      },
      "context_drift_prevention": {
        "task_boundaries": "This task ONLY implements core research command integration",
        "refer_to_other_tasks": {
          "Search system": "T-SEARCH-004",
          "Analysis system": "T-ANALYSIS-004",
          "Hypothesis system": "T-HYPOTH-004"
        },
        "max_file_changes": 5,
        "if_exceeds": "STOP and verify scope"
      },
      "milestone_metadata": {
        "milestone_id": "M4",
        "milestone_name": "Integration & Workflow",
        "is_milestone_critical": true,
        "can_defer": false,
        "milestone_position": 4
      }
    },
    {
      "id": "T-CMD-002",
      "title": "Implement FastAPI Backend Server",
      "prd_traceability": {
        "feature_id": "F004",
        "prd_lines": [107, 120],
        "original_requirement": "Research-focused commands for autonomous research"
      },
      "scope_boundaries": {
        "must_implement": [
          "FastAPI application with async endpoints",
          "Subagent coordination API endpoints",
          "Request/response models with Pydantic validation",
          "Error handling and status reporting,
          "API documentation with OpenAPI integration"
        ],
        "must_not_implement": [
          "User authentication and authorization",
          "Advanced API rate limiting beyond basic",
          "Web dashboard or UI components"
        ],
        "out_of_scope_check": "BLOCK if implementing web UI"
      },
      "documentation_context": {
        "primary_docs": [
          {
            "url": "https://fastapi.tiangolo.com/",
            "version": "0.115.0",
            "last_verified": "2025-11-09"
          }
        ],
        "version_locks": {
          "fastapi": "0.115.0",
          "pydantic": "2.9.2",
          "uvicorn": "0.32.1"
        },
        "forbidden_patterns": ["Synchronous endpoint implementations", "Missing validation", "No error handling"]
      },
      "hallucination_guards": {
        "verify_before_use": ["FastAPI async patterns", "Pydantic validation methods", "OpenAPI documentation generation"],
        "forbidden_assumptions": ["No endpoint conflicts", "Unlimited concurrent requests", "Perfect validation coverage"]
      },
      "context_drift_prevention": {
        "task_boundaries": "This task ONLY implements backend API server for subagent coordination",
        "refer_to_other_tasks": {
          "Command integration": "T-CMD-001",
          "Subagent coordination": "T-CMD-003"
        },
        "max_file_changes": 4,
        "if_exceeds": "STOP and verify scope"
      },
      "milestone_metadata": {
        "milestone_id": "M4",
        "milestone_name": "Integration & Workflow",
        "is_milestone_critical": true,
        "can_defer": false,
        "milestone_position": 4
      }
    },
    {
      "id": "T-CMD-003",
      "title": "Create Subagent Coordination and Workflow Management",
      "prd_traceability": {
        "feature_id": "F007",
        "prd_lines": [166, 185],
        "original_requirement": "Complete research workflow from search to hypothesis"
      },
      "scope_boundaries": {
        "must_implement": [
          "End-to-end research workflow orchestration,
          "Subagent task delegation and coordination,
          "Workflow state management and persistence,
          "Progress tracking and status reporting,
          "Error recovery and rollback mechanisms"
        ],
        "must_not_implement": [
          "Advanced workflow optimization algorithms",
          "Multi-user collaboration features",
          "Workflow template customization"
        ],
        "out_of_scope_check": "BLOCK if implementing advanced optimization"
      },
      "documentation_context": {
        "primary_docs": [
          {
            "url": "https://docs.crewai.com/",
            "version": "2025",
            "last_verified": "2025-11-09"
          }
        ],
        "version_locks": {
          "crewai": "0.75.0",
          "redis": "5.2.1",
          "pickle": "built-in"
        },
        "forbidden_patterns": ["Blocking workflow operations", "Missing state persistence", "No error recovery"]
      },
      "hallucination_guards": {
        "verify_before_use": ["CrewAI coordination patterns", "State management strategies", "Error recovery mechanisms"],
        "forbidden_assumptions": ["Perfect coordination", "No state corruption", "Complete error coverage"]
      },
      "context_drift_prevention": {
        "task_boundaries": "This task ONLY orchestrates existing subagents into complete workflow",
        "refer_to_other_tasks": {
          "All subagent systems": ["T-SEARCH-004", "T-ANALYSIS-004", "T-HYPOTH-004"],
          "Command system": "T-CMD-001",
          "API server": "T-CMD-002"
        },
        "max_file_changes": 5,
        "if_exceeds": "STOP and verify scope"
      },
      "milestone_metadata": {
        "milestone_id": "M4",
        "milestone_name": "Integration & Workflow",
        "is_milestone_critical": true,
        "can_defer": false,
        "milestone_position": 4
      }
    },
    {
      "id": "T-VAL-M4",
      "title": "Validate Milestone 4: Integration & Workflow",
      "type": "validation",
      "milestone_id": "M4",
      "validation_steps": [
        "Test all research slash commands through Claude Code",
        "Execute complete end-to-end research workflow",
        "Validate subagent coordination and state management",
        "Test error handling and recovery mechanisms",
        "Generate comprehensive system integration report",
        "PAUSE for human review"
      ],
      "success_criteria": {
        "commands_function_correctly": true,
        "end_to_end_workflow_completes": true,
        "subagent_coordination_works": true,
        "error_handling_robust": true,
        "system_ready_for_production": true
      },
      "rollback_point": false
    }
  ],
  "milestones": [
    {
      "id": "M1",
      "name": "Core Search Infrastructure",
      "description": "Paper search subagent with database integrations and vector search",
      "tasks": ["T-SEARCH-001", "T-SEARCH-002", "T-SEARCH-003", "T-SEARCH-004"],
      "launch_ready": true,
      "validation_criteria": {
        "api_integrations_work": "All three database APIs (Semantic Scholar, arXiv, PubMed) functional with rate limiting",
        "vector_search_accurate": "ChromaDB vector search provides relevant paper matches",
        "end_to_end_search_succeeds": "Complete search workflow from query to ranked results",
        "rate_limiting_respected": "All external API calls respect rate limits and implement backoff",
        "search_quality_acceptable": "Search results meet relevance and coverage quality thresholds"
      },
      "human_review_required": true,
      "rollback_point": true
    },
    {
      "id": "M2",
      "name": "Analysis & Reading System",
      "description": "Paper reader subagent with quality assessment and citation analysis",
      "tasks": ["T-ANALYSIS-001", "T-ANALYSIS-002", "T-ANALYSIS-003", "T-ANALYSIS-004", "T-ANALYSIS-005"],
      "launch_ready": true,
      "validation_criteria": {
        "pdf_parsing_accurate": "Research papers parsed correctly with section identification",
        "quality_assessment_reliable": "Paper quality scoring consistent and meaningful",
        "batch_processing_efficient": "Multiple papers processed efficiently with proper resource management",
        "citation_analysis_correct": "Citation networks constructed accurately from reference sections",
        "analysis_workflow_complete": "Complete paper analysis from raw PDF to structured insights"
      },
      "human_review_required": true,
      "rollback_point": true
    },
    {
      "id": "M3",
      "name": "Hypothesis Generation",
      "description": "Hypothesis subagent with knowledge synthesis and gap analysis",
      "tasks": ["T-HYPOTH-001", "T-HYPOTH-002", "T-HYPOTH-003", "T-HYPOTH-004"],
      "launch_ready": true,
      "validation_criteria": {
        "hypotheses_generated": "Novel, testable hypotheses generated from analyzed papers",
        "gaps_identified_accurately": "Research gaps identified match known domain knowledge",
        "knowledge_synthesis_effective": "Cross-paper insights combined meaningfully",
        "hypotheses_testable": "Generated hypotheses are feasible and evidence-based",
        "workflow_complete": "Complete workflow from paper analysis to hypothesis generation"
      },
      "human_review_required": true,
      "rollback_point": true
    },
    {
      "id": "M4",
      "name": "Integration & Workflow",
      "description": "End-to-end system integration with Claude Code commands and coordination",
      "tasks": ["T-CMD-001", "T-CMD-002", "T-CMD-003"],
      "launch_ready": true,
      "validation_criteria": {
        "commands_function_correctly": "All research slash commands work through Claude Code interface",
        "end_to_end_workflow_completes": "Complete research process from query to hypotheses",
        "subagent_coordination_works": "Three subagents coordinate effectively through workflow",
        "error_handling_robust": "System handles errors gracefully with appropriate recovery",
        "system_ready_for_production": "All components integrated and tested for production use"
      },
      "human_review_required": true,
      "rollback_point": false
    }
  ],
  "milestone_strategy": {
    "max_tasks_per_milestone": 5,
    "min_tasks_per_milestone": 3,
    "validation_frequency": "every_milestone",
    "human_review_points": ["M1", "M2", "M3", "M4"],
    "rollback_strategy": "milestone_based_with_full_recovery"
  },
  "scope_enforcement": {
    "max_tasks_per_feature": 5,
    "total_tasks": 20,
    "complexity_score": "high",
    "anti_creep_rules": [
      "No domain-specific logic in infrastructure tasks",
      "No external dependencies without API integration",
      "No features beyond PRD without explicit defer",
      "No task exceeds 6 file changes",
      "All features trace to PRD lines"
    ]
  }
}